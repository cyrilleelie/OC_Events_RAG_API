# Projet : POC Chatbot RAG - Puls-Events

Ce projet est un Proof of Concept (POC) d'un chatbot intelligent conÃ§u pour la sociÃ©tÃ© fictive Puls-Events. Le chatbot est capable de rÃ©pondre Ã  des questions en langage naturel sur des Ã©vÃ©nements culturels en s'appuyant sur une architecture RAG (Retrieval-Augmented Generation) et les donnÃ©es de la mÃ©tropole nantaise.

## ğŸ¯ Objectifs du POC

* **DÃ©montrer la faisabilitÃ© technique** d'un systÃ¨me RAG combinant recherche sÃ©mantique et LLM.
* **Fournir une API RESTful** robuste et exploitable pour tester la solution.
* **Ã‰valuer quantitativement** la qualitÃ© et la pertinence des rÃ©ponses gÃ©nÃ©rÃ©es.
* **Proposer un artefact conteneurisÃ©** avec Docker, prÃªt pour une dÃ©mo ou un dÃ©ploiement.

## âœ¨ Technologies UtilisÃ©es

* **Langage :** Python 3.12
* **Gestion de dÃ©pendances :** Poetry
* **API & Serveur :** Flask & Gunicorn
* **Interface de Chat :** Streamlit
* **Orchestration RAG :** LangChain
* **Base Vectorielle :** FAISS (CPU)
* **ModÃ¨le d'Embedding :** `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`
* **LLM de GÃ©nÃ©ration :**
    * **Local :** N'importe quel modÃ¨le compatible via **Ollama** (ex: Mistral 7B)
    * **API :** **Mistral AI** ou **Google Gemini**
* **Tests & Ã‰valuation :** Pytest & Ragas
* **Conteneurisation :** Docker

## ğŸ“‚ Structure du Projet

```
puls-events-rag/
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ README.md                 # Ce fichier
â”œâ”€â”€ app_streamlit.py          # Interface Streamlit
â”œâ”€â”€ api/
â”‚   â””â”€â”€ main.py               # API Flask
â”œâ”€â”€ evaluation_dataset.json   # Jeu de test pour l'Ã©valuation Ragas
â”œâ”€â”€ pyproject.toml            # DÃ©pendances et configuration Poetry
â”œâ”€â”€ poetry.lock
â”œâ”€â”€ requirements.txt          # GÃ©nÃ©rÃ© pour le build Docker
â”‚
â”œâ”€â”€ data/                     # DonnÃ©es CSV (ignorÃ© par Git)
â”‚
â”œâ”€â”€ reports/                  # Rapports d'Ã©valuation (ignorÃ© par Git)
â”‚
â”œâ”€â”€ scripts/                  # Scripts pour le pipeline de donnÃ©es et l'Ã©valuation
â”‚
â”œâ”€â”€ src/                      # Code source de la librairie principale
â”‚   â””â”€â”€ puls_events_rag/
â”‚       â”œâ”€â”€ config.py         # <-- FICHIER DE CONFIGURATION CENTRAL
â”‚       â””â”€â”€ rag_chain.py
â”‚
â”œâ”€â”€ tests/                    # Tests automatisÃ©s
â”‚
â””â”€â”€ vector_store/             # Index FAISS (ignorÃ© par Git)
```

## ğŸ› ï¸ Configuration du Chatbot

Ce projet est conÃ§u pour Ãªtre flexible. La configuration se fait via le fichier `src/puls_events_rag/config.py` et des variables d'environnement dans un fichier `.env`.

### Fichier `config.py`

Vous pouvez modifier `src/puls_events_rag/config.py` pour :
* **Changer la source de donnÃ©es :** Modifiez les variables `PORTAL_URL` et `DATASET_ID`.
* **Adapter le "rÃ´le" de l'assistant :** Modifiez la variable `PROMPT_TEMPLATE`.
* **Tester d'autres modÃ¨les d'embedding.**
* **Ajuster les performances du RAG :** Modifiez `CHUNK_SIZE`, `CHUNK_OVERLAP` et `RETRIEVER_K`.

### Fichier `.env` (SÃ©lecteur de LLM et ClÃ©s API)

CrÃ©ez un fichier `.env` Ã  la racine pour choisir le LLM et fournir les clÃ©s API nÃ©cessaires.

* **`LLM_PROVIDER`** : C'est la variable clÃ© qui dÃ©termine quel modÃ¨le sera utilisÃ©.
    * `"mistral_local"` : Pour utiliser un modÃ¨le via Ollama.
    * `"mistral_api"` : Pour utiliser l'API de Mistral AI.
    * `"google_api"` : Pour utiliser l'API de Google Gemini.
* **ClÃ©s API** : Fournissez les clÃ©s correspondantes au fournisseur choisi. `OPENAI_API_KEY` est requise pour le script d'Ã©valuation Ragas.

**Exemple pour utiliser Ollama en local :**
```env
LLM_PROVIDER="mistral_local"
OPENAI_API_KEY="sk-..." # Uniquement pour l'Ã©valuation Ragas
```

**Exemple pour utiliser l'API Google Gemini :**
```env
LLM_PROVIDER="google_api"
GOOGLE_API_KEY="AIzaSy..."
OPENAI_API_KEY="sk-..."
```

## ğŸš€ Installation et Pipeline de DonnÃ©es

**PrÃ©requis :** Git, Python 3.12+, Poetry, et Docker Desktop. Si vous utilisez un modÃ¨le local, **Ollama** doit Ãªtre installÃ© et en cours d'exÃ©cution.

1.  **Cloner le dÃ©pÃ´t :**
    ```bash
    git clone https://github.com/cyrilleelie/OC_Events_RAG_API
    cd OC_Events_RAG_API
    ```

2.  **Configurer le fichier `.env`** comme expliquÃ© dans la section prÃ©cÃ©dente.

3.  **Installer les dÃ©pendances Python :**
    ```bash
    poetry install
    ```

4.  **ExÃ©cuter le pipeline de donnÃ©es** (indispensable avant la premiÃ¨re utilisation) :
    * `poetry run python scripts/fetch_data.py`
    * `poetry run python scripts/create_vector_store.py`

## ğŸ’¬ Lancement de l'Interface de Chat (Streamlit)

C'est la mÃ©thode la plus simple pour interagir avec le chatbot en local. Le modÃ¨le utilisÃ© dÃ©pendra de la variable `LLM_PROVIDER` dans votre fichier `.env`.

```bash
poetry run streamlit run app_streamlit.py
```
Votre navigateur s'ouvrira automatiquement sur l'interface du chatbot.

## ğŸ³ Lancement via Docker (API)

Cette mÃ©thode lance l'API Flask conteneurisÃ©e.

1.  **Construire l'image Docker :**
    ```bash
    docker build -t puls-events-rag-api .
    ```

2.  **Lancer le conteneur Docker :**
    La commande varie selon le LLM que vous souhaitez utiliser.

    * **Mode 1 : ModÃ¨le local (Ollama) - (Par dÃ©faut)**
        *PrÃ©requis : Ollama doit tourner sur votre machine.*
        ```bash
        docker run -p 5001:5000 puls-events-rag-api
        ```

    * **Mode 2 : API Google Gemini**
        ```bash
        docker run -p 5001:5000 -e LLM_PROVIDER="google_api" --env-file .env puls-events-rag-api
        ```

    * **Mode 3 : API Mistral**
        ```bash
        docker run -p 5001:5000 -e LLM_PROVIDER="mistral_api" --env-file .env puls-events-rag-api
        ```
        
3.  **Tester l'API :**
    Envoyez une requÃªte `POST` Ã  `http://127.0.0.1:5001/ask` avec un corps JSON :
    ```json
    {
        "question": "Y a-t-il des expositions sur l'Ã®le de Nantes ?"
    }
    ```

## âœ… Tests et Ã‰valuation

1.  **Tests fonctionnels de l'API :**
    * Lancez l'API (localement ou via Docker).
    * Dans un autre terminal, lancez : `poetry run pytest`

2.  **Ã‰valuation de la qualitÃ© du RAG :**
    * **Principe :** Le script `scripts/evaluate_rag.py` compare les rÃ©ponses du chatbot (gÃ©nÃ©rÃ©es avec le `LLM_PROVIDER` de votre `.env`) Ã  une "vÃ©ritÃ© terrain" (`evaluation_dataset.json`). Il utilise Ragas pour noter la performance.
    * **PrÃ©requis :** L'Ã©valuation Ragas nÃ©cessite une clÃ© `OPENAI_API_KEY` dans votre fichier `.env` pour son LLM "juge".
    * **Lancer l'Ã©valuation :**
        ```bash
        poetry run python scripts/evaluate_rag.py
        ```
    * Les rÃ©sultats sont affichÃ©s et sauvegardÃ©s dans le dossier `reports/`.
